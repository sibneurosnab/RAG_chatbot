# 4. Логика диалога и промпт‑инжиниринг

> В этом документе описано, **как работает Telegram‑бот**:  
> • переключение между режимами **«Загрузка»** и **«Чат»**;  
> • хранение состояния пользователя в Supabase;  
> • RAG‑цикл генерации ответа (контекст диалога → поиск знаний → ответ);  
> • принципы системного промпта и ограничения на вызов поиска;  
> • ключевые преимущества решения.

---

## 4.1 Режимы работы Telegram‑бота: «Загрузка» и «Чат»

### 4.1.1 Переключение через Telegram‑кнопки  

| Шаг | Что делает пользователь | Что делает бот / n8n |
|-----|------------------------|-----------------------|
| ①   | Пользователь впервые открывает чат или сбрасывает состояние | Бот отправляет меню: «Выберите режим работы» с двумя inline‑кнопками **Чат** и **База** |
| ②   | Нажатие на кнопку `chat` или `documents` | `callback_query` попадает в workflow. `n8n` **upsert**‑ит запись в таблице `user_state` (`user_id`, `current_mode`, `session_id`, `updated_at`) |
| ③   | — | Бот подтверждает новый режим и выводит подсказку по использованию |
| ④   | В дальнейшем отправляет сообщения / файлы | Главный workflow читает `user_state.current_mode` и **ветвится**: → ветка индексации (режим *База*) или → ветка чата (режим *Чат*) |

> **Supabase :**  
> Таблица `user_state` хранит одно значение `current_mode` на `user_id`.  
> Чистый апдейт состояния выполняется через `POST .../user_state?on_conflict=user_id&prefer=resolution=merge-duplicates`.

### 4.1.2 Поведение ветки «Загрузка»  
Если `current_mode = "documents"`:

1. Входящие **URL / файлы / видео** классифицируются нодой `Classifier Code`.
2. Параллельные ветки:  
   • `link ↦ crawler & readability`  
   • `video ↦ whisper transcription`  
   • `file ↦ pdf/doc(x)/txt parser`
3. Выход каждой ветки ‑-> модуль **Late-Chunker RU** (чанкинг `paragraph + overlap`, эмбеддинг `jina-embeddings-v3` (`retrieval.passage`), *подробности см. «05 Late Chunking & Стратегии»*).
4. Фрагменты с метаданными (`source_type`, `source_identifier`) сохраняются в таблицы `documents_*` (Supabase + pgvector).

При этом **никаких ответов пользователю** в ветке загрузки не генерируется, бот лишь подтверждает, что документ принят и проиндексирован.

### 4.1.3 Поведение ветки «Чат»  
Если `current_mode = "chat"`, сообщение трактуется как вопрос. Логика RAG‑ответа описана в 4.2.

> ⚙️ **Сбросы**  
> • **Clean chat** удаляет все записи из `chat_messages` для пользователя.  
> • **Clean base** (опционально) удаляет документы пользователя из `documents_*`.

---

## 4.2 Ведение диалога: контекст и поиск по базе

1. **Извлечение истории.** `n8n` запрашивает из `chat_messages` последние 15 пар Q/A (≈ 30 строк).  
2. **Подготовка промпта.** В системный промпт помещаются:  
   * `context` – полученная история;  
   * `user_message` – *только* новое сообщение;  
   * описание **инструмента** `rag-db-query`.  
3. **Решение об обращении к базе.** LLM‑агент (GPT‑4 через OpenRouter) применяет правила:  
   * вызывать инструмент **один раз**, **только** если вопрос содержит индикаторы поиска (URL, имя файла, слова «проверь в базе», «cite», ключ `n8n` и т.д.);  
   * перед вызовом переформулировать запрос (убрать лишнее).  
4. **Векторный поиск.** Sub‑workflow `rag-db-query` выполняет:  
   * эмбеддинг запроса (`jina‑embeddings‑v3`, `retrieval.query`);  
   * ANN‑поиск (pgvector HNSW, top 30);  
   * реранжирование (`jina‑reranker-v2`, top 10).  
   Возвращается JSON со списком фрагментов и метаданными.  
5. **Генерация финального ответа.** LLM:  
   * опирается **только** на найденные фрагменты;  
   * формирует ответ на русском;  
   * добавляет блок граундации: «Информация взята из … (тип, имя/URL)».  
   Если результатов нет – отвечает из собственных знаний и *не* вставляет источник.  
6. **Логирование.** Пара Q/A пишется функцией `insert_and_trim_pairs` (Supabase), обрезая историю до 30 сообщений.

```mermaid
flowchart LR
  subgraph Telegram
    U([Пользователь])
    B([Бот])
  end
  subgraph Supabase
    US[(user_state)]
    CM[(chat_messages)]
    DOC[(documents_*)]
  end

  U -->|"first/open"| B
  B -->|"меню режимов"| U
  B -->|upsert| US
  U -->|"msg / file"| B
  B -->|"read mode"| US

  B -->|"IF → documents"| DOC
  B -->|"IF → chat"| CM

  B -->|history| CM
  B -->|"search (rag-db-query)"| DOC
  B -->|answer| U
  B -->|"log Q/A"| CM
  ```

## 4.3 LLM-агент и интеграция с поиском знаний

| Компонент      | Реализация                                                                                             |
|----------------|---------------------------------------------------------------------------------------------------------|
| **LLM**        | GPT-4 (OpenRouter)                                                                                      |
| **Tool-calling** | JSON-описание функции `rag-db-query`, передаваемое в поле `tools` AI-ноды                              |
| **Эмбеддинг**  | `jina-embeddings-v3` с LoRA-адаптерами `retrieval.query` / `retrieval.passage`                           |
| **Чанкинг**    | Кастомный `LateChunker` (режим `paragraph + overlap`, поддержка кириллицы)                              |
| **Векторная БД**| Supabase + pgvector (HNSW-индекс)                                                                       |
| **Реранкер**   | `jina-reranker-v2` (cross-encoder)                                                                      |

LLM действует как **агент**: решает, требуется ли вызвать `rag-db-query`. Попытка вызова перехватывается `n8n`, и запускается sub-workflow поиска (эмбеддинг → ANN → реранк).  

---

## 4.4 Системные промпты и правила ответов

* **Язык:** всегда русский.  
* **Стиль:** лаконичный, технически корректный; при непонятном вопросе – уточнить.  
* **Контекст:** использовать историю (`context`) для связности, но фокусироваться на текущем запросе.  
* **Инструмент:** доступен `rag-db-query`; вызывать ≤ 1 раз, **только** при явной необходимости (URL, «проверь в базе», ключ `n8n` и т. д.).  
* **После поиска:** формировать ответ, *перефразируя* найденные фрагменты; **обязательно** указывать источник (тип + имя/URL).  
* **Нет данных:** отвечать из внутренних знаний, не придумывать ссылки.  
* **Запреты:** галлюцинации, раскрытие инфраструктуры, дословное дублирование предыдущих ответов.

---

## 4.5 Ключевые особенности и преимущества

1. **Агентный RAG-подход** – LLM динамически решает, когда обращаться к базе знаний, экономя ресурсы и повышая релевантность.  
2. **Продуманный промпт-инжиниринг** – предотвращает галлюцинации, гарантирует цитирование источников и задаёт чёткий стиль ответов.  
3. **Многоходовой диалог с памятью** – история из `chat_messages` (до 30 сообщений) поддерживает связность разговоров.  
4. **Интуитивный UX** – пользователь одной кнопкой переключает режим **«Чат / База»**, не думая о внутренних пайплайнах.  
5. **Self-hosted инфраструктура** – поиск, хранение и реранк выполняются локально; при желании и LLM можно развернуть on-prem.  
6. **Полная поддержка русского языка** – кастомный sentence-splitter, адаптированный Late-Chunker и русские эмбеддинги `jina-embeddings-v3`.

---
Следующий документ: **05_chunking_strategy.md** – Late Chunking и стратегии чанкирования.
